## 作业三
### 基础作业
#### 配置lmdeploy运行环境
![Task 1-1](Task-1-1.png "Task 1-1")

#### 下载internlm-chat-1.8b模型
![Task 1-2](Task-1-2.png "Task 1-2")

#### 以命令行方式与模型对话
![Task 1-3](Task-1-3.png "Task 1-3")

### 进阶作业
#### 设置KV Cache最大占用比例为0.4，开启W4A16量化，以命令行方式与模型对话
![Task 2-1-1](Task-2-1-1.png "Task 2-1-1")

![Task 2-1-2](Task-2-1-2.png "Task 2-1-2")


#### 以API Server方式启动lmdeploy，开启 W4A16量化，调整KV Cache的占用比例为0.4，分别使用命令行客户端与Gradio网页客户端与模型对话
![Task 2-2-1](Task-2-2-1.png "Task 2-2-1")

![Task 2-2-2](Task-2-2-2.png "Task 2-2-2")


#### 使用W4A16量化，调整KV Cache的占用比例为0.4，使用Python代码集成的方式运行internlm2-chat-1.8b模型
![Task 2-3-1](Task-2-3-1.png "Task 2-3-1")

![Task 2-3-2](Task-2-3-2.png "Task 2-3-2")


#### 使用LMDeploy运行视觉多模态大模型llava gradio demo
![Task 2-4](Task-2-4.png "Task 2-4")


#### 将LMDeploy Web Demo部署到OpenXLab