## 笔记五 (LMDeploy量化部署LLM & VLM实践)

### 大模型部署背景
将训练好的模型放在特定环境中运行的过程\
**服务器端:** CPU部署、单GPU/TPU/NPU部署、多卡/集群部署\
**移动端/边缘端:** 移动机器人、手机 (考虑端侧计算能力)\
Model as a Service (MaaS)\
大模型部署面临的挑战:
  - **计算量巨大**: 前向推理时需要进行大量计算.\
InternLM2-20B: 生成128个token, 约进行5.2万亿次浮点运算.
  - **显存开销巨大**
  - **访存瓶颈**: 大模型推理是“访存密集”型任务, 硬件计算速度远快于显存带宽

### 大模型部署方法
**模型剪枝** (Pruning)\
移除模型中不必要或多余的组件, 比如参数, 使模型更加高效.
  - 非结构化剪枝 (SparseGPT, LoRAPrune, Wanda)
  - 结构化剪枝 (LLM-Pruner)

**知识蒸馏** (Knowledge Distillation, KD)\
通过性能更好、结构更复杂的教师模型来引导更轻量化的学生模型实现模型压缩.
  - 上下文学习 (ICL)
  - 思维链 (CoT)
  - 指令跟随 (IF)

**量化 (Quantization)**\
将传统的表示方法中的浮点数转换为整数或其它离散形式, 以减轻深度学习模型的存储和计算负担.
  - 量化感知训练 (QAT)
  - 量化感知微调 (QAF)
  - 训练后量化 (PTQ)

### LMDeploy简介
涵盖LLM任务的全套轻量化、部署和服务解决方案. 核心功能包含:
  - 高效推理: `lmdeploy chat -h`
  - 可靠量化: `lmdeploy lite -h`
  - 服务化部署: `lmdeploy serve -h`